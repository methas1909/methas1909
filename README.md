# Hi, I'm Methas Tupila üëã

üöÄ **Data Engineer | AI Engineer Enthusiast**

I'm a Data Engineer with over 3 years of experience designing and implementing robust data pipelines and ETL processes. Currently pursuing a Master's in Data Science and a Bachelor's in Law, I'm passionate about leveraging data to drive insights and building intelligent applications using AI and machine learning.

### üî≠ Current Focus

* Developing AI Agent prototypes using LangChain and OpenAI API.
* Enhancing data platforms with dbt, ClickHouse, and Azure Data Factory.

### üõ†Ô∏è Tech Stack

* **Languages:** Python, SQL
* **Data Engineering:** Apache Airflow, Kafka, dbt, PySpark, Delta Lake
* **Infrastructure:** Docker, Docker Compose, MinIO, PostgreSQL, ClickHouse
* **Cloud & Tools:** Azure, Git/GitHub, GitLab CI/CD

### üìÇ Notable Projects

* **[Kafka-Airflow-MinIO](https://github.com/methas1909/Kafka-Airflow-MinIO)**
  Prototype integrating Apache Kafka, Apache Airflow, MinIO, and PostgreSQL via Docker Compose. Demonstrates a simple producer-consumer pipeline storing JSON messages in MinIO. ([github.com](https://github.com/methas1909/Kafka-Airflow-MinIO))
* **[data-driven-lib](https://github.com/methas1909/data-driven-lib)**
  A Python library for building data-driven applications, including utilities for PySpark, MinIO, and Delta Lake. Features unit tests and packaging for distribution. ([github.com](https://github.com/methas1909/data-driven-lib))
* **[one-nab](https://github.com/methas1909/one-nab)**
  Comprehensive ETL framework for OneNab data warehouse, automating daily data ingestion, transformation, and master data maintenance using Airflow and PySpark. ([github.com](https://github.com/methas1909/one-nab))
* **[Cross-Check-Data-OBK](https://github.com/methas1909/Cross-Check-Data-OBK)**
  Jupyter Notebook for cross-checking OBK data, facilitating validation and anomaly detection in meter readings. ([github.com](https://github.com/methas1909/Cross-Check-Data-OBK))
* **[data-platform](https://github.com/methas1909/data-platform)**
  Docker Compose setup for local data platform, including Airflow and sample dataset structures for prototyping and learning. ([github.com](https://github.com/methas1909/data-platform))
* **[dbt-clickhouse-uk-price-paid](https://github.com/methas1909/dbt-clickhouse-uk-price-paid)**
  dbt project modeling UK Land Registry Price Paid Data on ClickHouse, providing transformations and reports on property transactions. ([github.com](https://github.com/methas1909/dbt-clickhouse-uk-price-paid))
* **[introducing-kafka](https://github.com/methas1909/introducing-kafka)**
  Workshop and documentation for getting started with Apache Kafka, using MkDocs for interactive learning. ([github.com](https://github.com/methas1909/introducing-kafka))
* **[craft\_data\_pipeline\_with\_apache\_airflow](https://github.com/methas1909/craft_data_pipeline_with_apache_airflow)**
  Boilerplate for creating Apache Airflow data pipelines, including directory structure and Docker Compose setup. ([github.com](https://github.com/methas1909/craft_data_pipeline_with_apache_airflow))

### üì´ Connect with me

* LinkedIn: [in/methas-tupila-dev](https://www.linkedin.com/in/methas-tupila-dev)

---

*Happy coding!*
